import argparse
import math
import os
#import platform
#import subprocess

import cv2
import numpy as np
import pandas as pd
import torch
from tqdm import tqdm
import openvino as ov

import audio
# from face_detect import face_rect
from models import Wav2Lip

from batch_face import RetinaFace
from time import time, sleep

import pyaudio

#import tkinter as tk
from PIL import Image, ImageTk

# Global variables for professional lip-sync
prev_frame = None
frame_counter = 0  # Counter to rotate through generated frames
all_generated_frames = []  # Store all frames from current batch
mouth_intensity = 0.55   # 55% AI, 45% original - balanced for natural movement
feather_radius = 15  # Smaller feathering for sharper edges

parser = argparse.ArgumentParser(description='Inference code to lip-sync videos in the wild using Wav2Lip models')

parser.add_argument('--checkpoint_path', type=str, default = "./Wav2Lip/checkpoints/wav2lip_gan.pth",
                    help='Name of saved checkpoint to load weights from', required=False)

parser.add_argument('--face', type=str, default="Elon_Musk.jpg",
                    help='Filepath of video/image that contains faces to use', required=False)
parser.add_argument('--audio', type=str, 
                    help='Filepath of video/audio file to use as raw audio source', required=False)
parser.add_argument('--outfile', type=str, help='Video path to save result. See default for an e.g.', 
                                default='results/result_voice.mp4')

parser.add_argument('--static', type=bool, 
                    help='If True, then use only first video frame for inference', default=False)
parser.add_argument('--fps', type=float, help='Can be specified only if input is a static image (default: 25)', 
                    default=15., required=False)

parser.add_argument('--pads', nargs='+', type=int, default=[0, 10, 0, 0], 
                    help='Padding (top, bottom, left, right). Please adjust to include chin at least')

parser.add_argument('--wav2lip_batch_size', type=int, help='Batch size for Wav2Lip model(s)', default=8)

parser.add_argument('--resize_factor', default=1, type=int,
             help='Reduce the resolution by this factor. Sometimes, best results are obtained at 480p or 720p')

parser.add_argument('--out_height', default=480, type=int,
            help='Output video height. Best results are obtained at 480 or 720')

parser.add_argument('--crop', nargs='+', type=int, default=[0, -1, 0, -1],
                    help='Crop video to a smaller region (top, bottom, left, right). Applied after resize_factor and rotate arg. ' 
                    'Useful if multiple face present. -1 implies the value will be auto-inferred based on height, width')

parser.add_argument('--box', nargs='+', type=int, default=[-1, -1, -1, -1], 
                    help='Specify a constant bounding box for the face. Use only as a last resort if the face is not detected.'
                    'Also, might work only if the face is not moving around much. Syntax: (top, bottom, left, right).')

parser.add_argument('--rotate', default=False, action='store_true',
                    help='Sometimes videos taken from a phone can be flipped 90deg. If true, will flip video right by 90deg.'
                    'Use if you get a flipped result, despite feeding a normal looking video')

parser.add_argument('--nosmooth', default=False, action='store_true',
                    help='Prevent smoothing face detections over a short temporal window')



class Wav2LipInference:
    
    def __init__(self, args) -> None:
        
        self.CHUNK = 1024 # piece of audio data, no of frames per buffer during audio capture, large chunk size reduces computational overhead but may add latency and vise versa
        self.FORMAT = pyaudio.paInt16
        self.CHANNELS = 1 # no of audio channels, 1 means monaural audio
        self.RATE = 16000 # sample rate of the audio stream, 16000 samples/second
        self.RECORD_SECONDS = 0.5 # time for which we capture the audio
        self.mel_step_size = 16 # mel freq step size
        self.audio_fs = 16000    # Sample rate
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.args = args

        print('Using {} for inference.'.format(self.device))

        self.model = self.load_model()
        self.detector = self.load_batch_face_model()

        self.face_detect_cache_result = None
        self.img_tk = None


    def load_wav2lip_openvino_model(self):
        '''
        func to load open vino model
        for wav2lip
        '''

        print("Calling wav2lip openvino model for inference...")
        core = ov.Core()
        devices = core.available_devices
        print(devices[0])
        model = core.read_model(model=os.path.join("./openvino_model/", "wav2lip_openvino_model.xml"))
        compiled_model = core.compile_model(model = model, device_name = devices[0])
        return compiled_model
    
    def load_model_weights(self, checkpoint_path):

        if self.device == 'cuda':
            checkpoint = torch.load(checkpoint_path)
        else:
            checkpoint = torch.load(checkpoint_path,
                                    map_location=lambda storage, loc: storage)
        return checkpoint

    def load_wav2lip_model(self, checkpoint_path):

        model = Wav2Lip()
        print("Load checkpoint from: {}".format(checkpoint_path))
        checkpoint = self.load_model_weights(checkpoint_path)
        s = checkpoint["state_dict"]
        new_s = {}
        for k, v in s.items():
            new_s[k.replace('module.', '')] = v
        model.load_state_dict(new_s)

        model = model.to(self.device)
        return model.eval()

    def load_model(self):

        if self.device=='cpu':
            return self.load_wav2lip_openvino_model()
        else:
            return self.load_wav2lip_model(self.args.checkpoint_path)

    def load_batch_face_model(self):

        if self.device=='cpu':
            return RetinaFace(gpu_id=-1, model_path="checkpoints/mobilenet.pth", network="mobilenet")
        else:
            return RetinaFace(gpu_id=0, model_path="checkpoints/mobilenet.pth", network="mobilenet")
            
    def face_rect(self, images):

        face_batch_size = 64 * 8
        num_batches = math.ceil(len(images) / face_batch_size)
        prev_ret = None
        for i in range(num_batches):
            batch = images[i * face_batch_size: (i + 1) * face_batch_size]
            all_faces = self.detector(batch)  # return faces list of all images
            for faces in all_faces:
                if faces:
                    box, landmarks, score = faces[0]
                    prev_ret = tuple(map(int, box))
                yield prev_ret

    def record_audio_stream(self, stream):

        stime = time()
        print("Recording audio ...")
        frames = []
        for i in range(0, int(self.RATE / self.CHUNK * self.RECORD_SECONDS)):
            frames.append(stream.read(self.CHUNK, exception_on_overflow=False))  # Append audio data as numpy array

        print("Finished recording for curr time stamp ....")
        print("recording time, ", time() - stime) 
        
        #audio_data = np.concatenate(frames)  # Combine all recorded frames into a single numpy array
        audio_data = np.frombuffer(b''.join(frames), dtype=np.int16)
        return audio_data

    def get_mel_chunks(self, audio_data):

        # Now you can perform mel chunk extraction directly on audio_data
        # Assuming you have functions audio.load_wav and audio.melspectrogram defined elsewhere in your code
        stime = time()
        # Example:
        wav = audio_data
        mel = audio.melspectrogram(wav)
        print(mel.shape, time()-stime)

        # convert to mel chunks
        if np.isnan(mel.reshape(-1)).sum() > 0:
            raise ValueError('Mel contains nan! Using a TTS voice? Add a small epsilon noise to the wav file and try again')
        
        stime = time()
        mel_chunks = []
        mel_idx_multiplier = 80./self.args.fps
        i = 0
        while 1:
            start_idx = int(i * mel_idx_multiplier)
            if start_idx + self.mel_step_size > len(mel[0]):
                mel_chunks.append(mel[:, len(mel[0]) - self.mel_step_size:])
                break
            mel_chunks.append(mel[:, start_idx : start_idx + self.mel_step_size])
            i += 1

        print("Length of mel chunks: {}".format(len(mel_chunks)))
        print(time()-stime)

        return mel_chunks

    def get_smoothened_boxes(self, boxes, T):

        for i in range(len(boxes)):
            if i + T > len(boxes):
                window = boxes[len(boxes) - T:]
            else:
                window = boxes[i : i + T]
            boxes[i] = np.mean(window, axis=0)
        return boxes

    def face_detect(self, images):

        results = []
        pady1, pady2, padx1, padx2 = self.args.pads

        s = time()

        for image, rect in zip(images, self.face_rect(images)):
            if rect is None:
                print("Face was not detected...")
                cv2.imwrite('temp/faulty_frame.jpg', image) # check this frame where the face was not detected.
                raise ValueError('Face not detected! Ensure the video contains a face in all the frames.')

            y1 = max(0, rect[1] - pady1)
            y2 = min(image.shape[0], rect[3] + pady2)
            x1 = max(0, rect[0] - padx1)
            x2 = min(image.shape[1], rect[2] + padx2)

            results.append([x1, y1, x2, y2])

        print('face detect time:', time() - s)

        boxes = np.array(results)
        if not self.args.nosmooth: boxes = self.get_smoothened_boxes(boxes, T=5)
        results = [[image[y1: y2, x1:x2], (y1, y2, x1, x2)] for image, (x1, y1, x2, y2) in zip(images, boxes)]

        return results

    def datagen(self, frames, mels):

        img_batch, mel_batch, frame_batch, coords_batch = [], [], [], []

        if self.args.box[0] == -1:
            if not self.args.static:
                face_det_results = self.face_detect(frames) # BGR2RGB for CNN face detection
            else:
                face_det_results = self.face_detect_cache_result # use cached result #face_detect([frames[0]])
        else:
            print('Using the specified bounding box instead of face detection...')
            y1, y2, x1, x2 = self.args.box
            face_det_results = [[f[y1: y2, x1:x2], (y1, y2, x1, x2)] for f in frames]

        for i, m in enumerate(mels):
            
            idx = 0 if self.args.static else i%len(frames)
            frame_to_save = frames[idx].copy()
            face, coords = face_det_results[idx].copy()

            face = cv2.resize(face, (self.args.img_size, self.args.img_size))

            img_batch.append(face)
            mel_batch.append(m)
            frame_batch.append(frame_to_save)
            coords_batch.append(coords)

            if len(img_batch) >= self.args.wav2lip_batch_size:
                img_batch, mel_batch = np.asarray(img_batch), np.asarray(mel_batch)

                img_masked = img_batch.copy()
                img_masked[:, self.args.img_size//2:] = 0

                img_batch = np.concatenate((img_masked, img_batch), axis=3) / 255.
                mel_batch = np.reshape(mel_batch, [len(mel_batch), mel_batch.shape[1], mel_batch.shape[2], 1])

                yield img_batch, mel_batch, frame_batch, coords_batch
                img_batch, mel_batch, frame_batch, coords_batch = [], [], [], []

        # if there are any other batches
        if len(img_batch) > 0:
            img_batch, mel_batch = np.asarray(img_batch), np.asarray(mel_batch)

            img_masked = img_batch.copy()
            img_masked[:, self.args.img_size//2:] = 0

            img_batch = np.concatenate((img_masked, img_batch), axis=3) / 255.
            mel_batch = np.reshape(mel_batch, [len(mel_batch), mel_batch.shape[1], mel_batch.shape[2], 1])

            yield img_batch, mel_batch, frame_batch, coords_batch


def update_frames(full_frames, stream, inference_pipline, original_frame=None):
    global prev_frame, frame_counter, all_generated_frames

    stime = time()
    # convert recording to mel chunks
    audio_data = inference_pipline.record_audio_stream(stream)

    # Check for silence - close mouth during pauses
    # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð±Ð¾Ð»ÐµÐµ Ð²Ñ‹ÑÐ¾ÐºÐ¸Ð¹ Ð¿Ð¾Ñ€Ð¾Ð³ Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð½Ðµ Ñ€ÐµÐ°Ð³Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð½Ð° Ñ„Ð¾Ð½Ð¾Ð²Ñ‹Ð¹ ÑˆÑƒÐ¼
    audio_rms = np.sqrt(np.mean(audio_data.astype(np.float32)**2))

    # Ð‘Ð¾Ð»ÐµÐµ Ð²Ñ‹ÑÐ¾ÐºÐ¸Ð¹ Ð¿Ð¾Ñ€Ð¾Ð³ - Ñ€ÐµÐ°Ð³Ð¸Ñ€ÑƒÐµÑ‚ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð½Ð° ÑÐ²Ð½ÑƒÑŽ Ñ€ÐµÑ‡ÑŒ
    silence_threshold = 500  # ÐŸÐ¾Ð²Ñ‹ÑˆÐµÐ½ Ñ 150 Ð´Ð¾ 500 Ð´Ð»Ñ Ð¸Ð³Ð½Ð¾Ñ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ñ„Ð¾Ð½Ð¾Ð²Ð¾Ð³Ð¾ ÑˆÑƒÐ¼Ð°

    # Ð”Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð°Ñ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ°: ÐµÑÑ‚ÑŒ Ð»Ð¸ Ñ€ÐµÐ°Ð»ÑŒÐ½Ð°Ñ Ñ€ÐµÑ‡ÐµÐ²Ð°Ñ Ð°ÐºÑ‚Ð¸Ð²Ð½Ð¾ÑÑ‚ÑŒ
    # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð¿Ð¸ÐºÐ¾Ð²Ñ‹Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ (Ð´Ð¾Ð»Ð¶Ð½Ñ‹ Ð±Ñ‹Ñ‚ÑŒ Ð²Ñ‹ÑˆÐµ ÑÑ€ÐµÐ´Ð½ÐµÐ³Ð¾ Ð² 3+ Ñ€Ð°Ð·Ð°)
    audio_peak = np.max(np.abs(audio_data.astype(np.float32)))
    speech_detected = (audio_rms > silence_threshold) and (audio_peak > audio_rms * 3)

    if not speech_detected and original_frame is not None:
        # ÐÐµÑ‚ Ñ€ÐµÑ‡Ð¸ - Ð·Ð°ÐºÑ€Ñ‹Ð²Ð°ÐµÐ¼ Ñ€Ð¾Ñ‚ (Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼ Ð¾Ñ€Ð¸Ð³Ð¸Ð½Ð°Ð»ÑŒÐ½Ð¾Ðµ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ)
        print(f"ðŸ”‡ Silence (RMS: {audio_rms:.0f}, Peak: {audio_peak:.0f}) - mouth CLOSED")

        # Clear generated frames cache on silence
        all_generated_frames = []
        frame_counter = 0
        prev_frame = original_frame.copy()

        _, buffer = cv2.imencode('.jpg', original_frame, [cv2.IMWRITE_JPEG_QUALITY, 95])
        buffer = np.array(buffer)
        buffer = buffer.tobytes()
        return (b'--frame\r\n'
               b'Content-Type: image/jpeg\r\n\r\n' + buffer + b'\r\n')

    print(f"ðŸ—£ï¸  Speech detected (RMS: {audio_rms:.0f}, Peak: {audio_peak:.0f}) - mouth OPENING")
    mel_chunks = inference_pipline.get_mel_chunks(audio_data)
    print(f"Time to process audio input {time()-stime}")

    # Repeat the single frame for each mel chunk if we only have one frame
    if len(full_frames) == 1:
        full_frames = full_frames * len(mel_chunks)
    else:
        full_frames = full_frames[:len(mel_chunks)]
    
    batch_size = inference_pipline.args.wav2lip_batch_size
    gen = inference_pipline.datagen(full_frames.copy(), mel_chunks.copy())
   
    s = time()    

    for i, (img_batch, mel_batch, frames, coords) in enumerate(tqdm(gen,
                                        total=int(np.ceil(float(len(mel_chunks))/batch_size)))):
        
        if inference_pipline.device=='cpu':
            img_batch = np.transpose(img_batch, (0, 3, 1, 2))
            mel_batch = np.transpose(mel_batch, (0, 3, 1, 2))
            print(img_batch.shape, mel_batch.shape)
            pred = inference_pipline.model([mel_batch, img_batch])['output']
        else:
            img_batch = torch.FloatTensor(np.transpose(img_batch, (0, 3, 1, 2))).to(inference_pipline.device)
            mel_batch = torch.FloatTensor(np.transpose(mel_batch, (0, 3, 1, 2))).to(inference_pipline.device)
            print(img_batch.shape, mel_batch.shape)
            with torch.no_grad():
                pred = inference_pipline.model(mel_batch, img_batch)


        print(f"[DEBUG] Model output shape: {pred.shape}")
        print(f"[DEBUG] Model output range: [{pred.min():.3f}, {pred.max():.3f}]")
        pred = pred.transpose(0, 2, 3, 1) * 255.
        print(f"[DEBUG] After scaling range: [{pred.min():.1f}, {pred.max():.1f}]")

        # Generate all frames professionally - clean, sharp output
        generated_frames = []

        for idx, (p, f, c) in enumerate(zip(pred, frames, coords)):
            y1, y2, x1, x2 = c

            # OFFICIAL OpenVINO approach: Model already outputs BGR format
            # No RGB2BGR conversion needed!
            p = p.astype(np.uint8)
            p = cv2.resize(p, (x2 - x1, y2 - y1))

            # Debug first frame
            if idx == 0:
                print(f"[DEBUG] Model output (BGR): {p[p.shape[0]//2, p.shape[1]//2, :]}")
                print(f"[DEBUG] Original face (BGR): {f[y1:y2, x1:x2][p.shape[0]//2, p.shape[1]//2, :]}")

            # Direct replacement (official method) - maximum lip sync accuracy
            f[y1:y2, x1:x2] = p

            generated_frames.append(f.copy())

        # CRITICAL FIX: Rotate through ALL frames, not just middle one
        if len(generated_frames) > 0:
            all_generated_frames = generated_frames

            # Cycle through frames to ensure continuous mouth movement
            frame_to_return = all_generated_frames[frame_counter % len(all_generated_frames)]
            frame_counter += 1  # Increment to get next frame on next call

            prev_frame = frame_to_return.copy()

            # High-quality JPEG encoding for professional output
            _, buffer = cv2.imencode('.jpg', frame_to_return, [cv2.IMWRITE_JPEG_QUALITY, 95])
            buffer = np.array(buffer)
            buffer = buffer.tobytes()

            return (b'--frame\r\n'
                   b'Content-Type: image/jpeg\r\n\r\n' + buffer + b'\r\n')
                    

def main(imagefilepath, flag):

    args = parser.parse_args()
    args.img_size = 96
    args.face = imagefilepath
    inference_pipline = Wav2LipInference(args)

    if os.path.isfile(args.face) and args.face.split('.')[-1] in ['jpg', 'png', 'jpeg']:
        args.static = True

    if not os.path.isfile(args.face):
        raise ValueError('--face argument must be a valid path to video/image file')

    elif args.face.split('.')[-1] in ['jpg', 'png', 'jpeg']:
        full_frames = [cv2.imread(args.face)]
        fps = args.fps

    else:
        video_stream = cv2.VideoCapture(args.face)
        fps = video_stream.get(cv2.CAP_PROP_FPS)

        print('Reading video frames...')

        full_frames = []
        while 1:
            still_reading, frame = video_stream.read()
            if not still_reading:
                video_stream.release()
                break

            aspect_ratio = frame.shape[1] / frame.shape[0]
            frame = cv2.resize(frame, (int(args.out_height * aspect_ratio), args.out_height))
            # if args.resize_factor > 1:
            #     frame = cv2.resize(frame, (frame.shape[1]//args.resize_factor, frame.shape[0]//args.resize_factor))

            if args.rotate:
                frame = cv2.rotate(frame, cv2.cv2.ROTATE_90_CLOCKWISE)

            y1, y2, x1, x2 = args.crop
            if x2 == -1: x2 = frame.shape[1]
            if y2 == -1: y2 = frame.shape[0]

            frame = frame[y1:y2, x1:x2]

            full_frames.append(frame)

    print ("Number of frames available for inference: "+str(len(full_frames)))

    p = pyaudio.PyAudio()
    stream = p.open(format=inference_pipline.FORMAT,
                    channels=inference_pipline.CHANNELS,
                    rate=inference_pipline.RATE,
                    input=True,
                    frames_per_buffer=inference_pipline.CHUNK)
    
    inference_pipline.face_detect_cache_result = inference_pipline.face_detect([full_frames[0]])

    # Store original frame for silence detection (to show closed mouth when not speaking)
    original_frame = full_frames[0].copy()

    while True:
        if not flag:
            stream.stop_stream()
            stream.close()
            p.terminate()
            return b""
        print(f"Model inference flag {flag}")
        yield update_frames(full_frames, stream, inference_pipline, original_frame)
    

